{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r55IHvfa1xuW"
      },
      "outputs": [],
      "source": [
        "# INSTALL REQUIRED PACKAGES\n",
        "\n",
        "print(\" Installing required packages\")\n",
        "# Upgrade pip\n",
        "!pip install --upgrade pip --quiet\n",
        "\n",
        "print(\"Installing scikit-learn\")\n",
        "!pip install scikit-learn==1.5.0 --quiet\n",
        "\n",
        "# --- Skipped TA-Lib installation due to issues ---\n",
        "\n",
        "# print(\"\\nAttempting TA-Lib installation\")\n",
        "# !apt-get update -qq\n",
        "# !apt-get install -y build-essential python3-dev ta-lib libta-lib0-dev --quiet\n",
        "# !pip install TA-Lib --quiet\n",
        "\n",
        "# --- Install scikeras for Keras-Scikit-learn integration ---\n",
        "print(\"Installing scikeras for Keras-Scikit-learn integration\")\n",
        "!pip install scikeras[tensorflow] --quiet\n",
        "\n",
        "# --- Install other core packages(order imp) ---\n",
        "print(\"Installing other core Python packages\")\n",
        "!pip install yfinance tensorflow pandas numpy matplotlib seaborn plotly xgboost shap --quiet\n",
        "\n",
        "print(\"\\n All required packages installation attempts completed.\")\n",
        "\n",
        "# IMPORT ESSENTIAL LIBRARIES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data fetching library\n",
        "import yfinance as yf\n",
        "\n",
        "# For preprocessing and scaling\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For traditional ML models\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# For Deep Learning models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Import KerasRegressor from scikeras.wrappers\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "# For model evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# For hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# For explainability\n",
        "import shap\n",
        "\n",
        "#  Attempt to import TA-Lib here, it will fail gracefully if not installed\n",
        "try:\n",
        "    import talib as ta\n",
        "    TALIB_AVAILABLE = True\n",
        "    print(\"TA-Lib successfully imported for technical indicator calculations.\")\n",
        "except ImportError:\n",
        "    TALIB_AVAILABLE = False\n",
        "    print(\"TA-Lib not found. Manual calculations will be used for technical indicators.\")\n",
        "\n",
        "\n",
        "print(\"Essential libraries imported.\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Setup Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE7mqAp_6HVo"
      },
      "outputs": [],
      "source": [
        "print(\"Data Collection Setup\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# DATA COLLECTION FUNCTION\n",
        "\n",
        "def collect_stock_data(symbol, period=\"2y\", interval=\"1d\"):\n",
        "    \"\"\"\n",
        "    Collect stock data from Yahoo Finance.\n",
        "\n",
        "    Parameters:\n",
        "    - symbol: Stock symbol (e.g., 'AAPL', 'MSFT').\n",
        "    - period: Time period (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max).\n",
        "    - interval: Data interval (1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo).\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with stock data, or None if an error occurs or no data is found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Collecting data for {symbol} (Period: {period}, Interval: {interval})\")\n",
        "\n",
        "        ticker = yf.Ticker(symbol)\n",
        "\n",
        "        data = ticker.history(period=period, interval=interval)\n",
        "\n",
        "        if data.empty:\n",
        "            print(f\" No data found for {symbol} with the specified period/interval.\")\n",
        "            return None\n",
        "\n",
        "        data.index = pd.to_datetime(data.index)\n",
        "        data = data.sort_index()\n",
        "\n",
        "        print(f\"Successfully collected {len(data)} records for {symbol}.\")\n",
        "        print(f\"Date range: {data.index.min().strftime('%Y-%m-%d')} to {data.index.max().strftime('%Y-%m-%d')}.\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting data for {symbol}: {str(e)}.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Define stocks to analyze\n",
        "STOCK_SYMBOLS = [\n",
        "    'AAPL',  # Apple\n",
        "    'MSFT',  # Microsoft\n",
        "    'GOOGL', # Google\n",
        "    'AMZN',  # Amazon\n",
        "    'TSLA'   # Tesla\n",
        "]\n",
        "\n",
        "PRIMARY_STOCK = 'AAPL'\n",
        "\n",
        "print(f\"\\nCollecting data for the primary stock: {PRIMARY_STOCK}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "stock_data = collect_stock_data(PRIMARY_STOCK, period=\"3y\", interval=\"1d\")\n",
        "\n",
        "if stock_data is not None:\n",
        "    print(f\"\\nData Overview for {PRIMARY_STOCK}:\")\n",
        "    print(f\"Shape: {stock_data.shape}\")\n",
        "    print(f\"Columns: {list(stock_data.columns)}\")\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(stock_data.head())\n",
        "\n",
        "    print(\"\\nLast 5 rows:\")\n",
        "    print(stock_data.tail())\n",
        "\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(stock_data.describe())\n",
        "else:\n",
        "    print(\"\\n Failed to collect data for the primary stock. Please check your internet connection, stock symbol, or period/interval.\")\n",
        "\n",
        "print(\"\\n Data Collection Successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFI8QU_36vZx"
      },
      "outputs": [],
      "source": [
        "print(\"Data Quality Check\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "PROCEED_ON_QUALITY_ISSUES = False # <--- ADJUST THIS FLAG AS NEEDED\n",
        "\n",
        "def perform_data_quality_check(data, symbol):\n",
        "    \"\"\"\n",
        "    Perform comprehensive data quality checks.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Stock data DataFrame.\n",
        "    - symbol: Stock symbol for reporting.\n",
        "\n",
        "    Returns:\n",
        "    - quality_report: Dictionary with quality metrics.\n",
        "    - is_quality_good: Boolean indicating if data passes quality checks.\n",
        "    \"\"\"\n",
        "\n",
        "    quality_report = {}\n",
        "    issues = []\n",
        "\n",
        "    print(f\"Performing quality check for {symbol}\")\n",
        "\n",
        "    missing_values = data.isnull().sum()\n",
        "    missing_percentage = (missing_values / len(data)) * 100\n",
        "\n",
        "    quality_report['missing_values'] = missing_values\n",
        "    quality_report['missing_percentage'] = missing_percentage\n",
        "\n",
        "    print(\"\\nMissing Values Analysis:\")\n",
        "    if missing_values.sum() == 0:\n",
        "        print(\"  No missing values found.\")\n",
        "    else:\n",
        "        for col in data.columns:\n",
        "            if missing_values[col] > 0:\n",
        "                print(f\"    {col}: {missing_values[col]} ({missing_percentage[col]:.2f}%) missing values detected.\")\n",
        "                issues.append(f\"Missing values in {col}\")\n",
        "\n",
        "\n",
        "    # Data completeness\n",
        "    total_records = len(data)\n",
        "    quality_report['total_records'] = total_records\n",
        "\n",
        "    print(f\"\\n Data Completeness:\")\n",
        "    print(f\"   Total records: {total_records}\")\n",
        "\n",
        "    if total_records < 100: # threshold for sufficient data\n",
        "        issues.append(\"Insufficient data (less than 100 records)\")\n",
        "        print(\"  Warning: Less than 100 records available for analysis.\")\n",
        "    else:\n",
        "        print(\"  Sufficient data for analysis.\")\n",
        "\n",
        "    # Data consistency\n",
        "    print(f\"\\n Data Consistency Checks:\")\n",
        "\n",
        "    price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    negative_values_found = False\n",
        "    for col in price_cols:\n",
        "        if col in data.columns and (data[col] < 0).any().any():\n",
        "            issues.append(f\"Negative values found in {col}.\")\n",
        "            print(f\"  Negative values detected in {col}.\")\n",
        "            negative_values_found = True\n",
        "    if not negative_values_found:\n",
        "        print(\"   All price and volume values are non-negative.\")\n",
        "\n",
        "    invalid_high_low = (data['High'] < data['Low']).sum()\n",
        "    if invalid_high_low > 0:\n",
        "        issues.append(f\"{invalid_high_low} records with High < Low.\")\n",
        "        print(f\"   {invalid_high_low} records with invalid High/Low relationship.\")\n",
        "    else:\n",
        "        print(\"  High/Low relationship is valid.\")\n",
        "\n",
        "    close_out_of_range = ((data['Close'] > data['High']) | (data['Close'] < data['Low'])).sum()\n",
        "    if close_out_of_range > 0:\n",
        "        issues.append(f\"{close_out_of_range} records with Close outside High/Low range.\")\n",
        "        print(f\"   {close_out_of_range} records with Close outside High/Low range.\")\n",
        "    else:\n",
        "        print(\"   Close prices are within High/Low range.\")\n",
        "\n",
        "    print(f\"\\n Data Distribution Analysis:\")\n",
        "\n",
        "    data_temp = data.copy()\n",
        "    if not data_temp['Close'].shift(1).isnull().all():\n",
        "        data_temp['Price_Change_Pct'] = (data_temp['Close'].pct_change() * 100)\n",
        "        extreme_changes = (abs(data_temp['Price_Change_Pct']) > 50).sum()\n",
        "        if extreme_changes > 0:\n",
        "            print(f\"    {extreme_changes} days with extreme price changes (>50%).\")\n",
        "            quality_report['extreme_changes'] = extreme_changes\n",
        "            issues.append(\"Extreme price changes detected.\")\n",
        "        else:\n",
        "            print(\"  No extreme price changes detected (threshold: >50%).\")\n",
        "    else:\n",
        "        print(\"   Skipping extreme price change check due to insufficient data for pct_change (perhaps too few records).\")\n",
        "\n",
        "\n",
        "    # Date continuity\n",
        "    print(f\"\\n Date Continuity Check:\")\n",
        "    date_gaps = []\n",
        "    dates = data.index.to_list()\n",
        "\n",
        "    for i in range(1, len(dates)):\n",
        "        gap = (dates[i] - dates[i-1]).days\n",
        "        if gap > 7:\n",
        "            date_gaps.append(gap)\n",
        "\n",
        "    if date_gaps:\n",
        "        print(f\"   Found {len(date_gaps)} significant date gaps (more than 7 days).\")\n",
        "        quality_report['date_gaps'] = date_gaps\n",
        "        issues.append(\"Significant date gaps detected.\")\n",
        "    else:\n",
        "        print(\"   No significant date gaps found.\")\n",
        "\n",
        "    quality_report['issues'] = issues\n",
        "    is_quality_good = len(issues) == 0\n",
        "\n",
        "    print(f\"\\n Quality Check Summary:\")\n",
        "    if is_quality_good:\n",
        "        print(\"   Data quality is GOOD - Ready for preprocessing.\")\n",
        "    else:\n",
        "        print(\"   Data quality issues detected:\")\n",
        "        for issue in issues:\n",
        "            print(f\"      - {issue}\")\n",
        "\n",
        "    return quality_report, is_quality_good\n",
        "\n",
        "# PERFORM QUALITY CHECK\n",
        "\n",
        "if 'stock_data' in locals() and stock_data is not None and 'PRIMARY_STOCK' in locals():\n",
        "    quality_report, data_quality_passed = perform_data_quality_check(stock_data, PRIMARY_STOCK)\n",
        "\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # Price trends\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(stock_data.index, stock_data['Close'], label='Close Price', color='blue', linewidth=1.5)\n",
        "    plt.title(f'{PRIMARY_STOCK} - Price Trend', fontsize=14)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Price ($)', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Volume trends\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(stock_data.index, stock_data['Volume'], label='Volume', color='green', linewidth=1.5)\n",
        "    plt.title(f'{PRIMARY_STOCK} - Volume Trend', fontsize=14)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Volume', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Missing values heatmap\n",
        "    plt.subplot(2, 3, 3)\n",
        "    missing_data = stock_data.isnull()\n",
        "    if missing_data.any().any():\n",
        "        sns.heatmap(missing_data, cbar=True, yticklabels=False, cmap='viridis')\n",
        "        plt.title('Missing Values Heatmap', fontsize=14)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=16, color='darkgreen')\n",
        "        plt.title('Missing Values Check', fontsize=14)\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Price distribution\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.hist(stock_data['Close'].dropna(), bins=50, alpha=0.8, color='skyblue', edgecolor='gray')\n",
        "    plt.title('Close Price Distribution', fontsize=14)\n",
        "    plt.xlabel('Price ($)', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Daily returns distribution\n",
        "    plt.subplot(2, 3, 5)\n",
        "    daily_returns = stock_data['Close'].pct_change().dropna() * 100\n",
        "    plt.hist(daily_returns, bins=50, alpha=0.8, color='orange', edgecolor='gray')\n",
        "    plt.title('Daily Returns Distribution (%)', fontsize=14)\n",
        "    plt.xlabel('Daily Return (%)', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Quality summary (text based)\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.axis('off')\n",
        "\n",
        "    issues_list_formatted = '- ' + '\\n- '.join(quality_report['issues']) if quality_report['issues'] else 'None'\n",
        "\n",
        "    quality_text = (\n",
        "        \"Overall Data Quality Report:\\n\\n\"\n",
        "        f\"Total Records: {quality_report['total_records']}\\n\"\n",
        "        f\"Total Missing Values: {quality_report['missing_values'].sum()}\\n\\n\"\n",
        "        f\"Status: {' PASSED' if data_quality_passed else 'ISSUES FOUND'}\\n\\n\"\n",
        "        \"Detected Issues:\\n\"\n",
        "        f\"{issues_list_formatted}\\n\"\n",
        "    )\n",
        "\n",
        "    plt.text(0.05, 0.95, quality_text.strip(), fontsize=12, verticalalignment='top',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", lw=1, alpha=0.8))\n",
        "    plt.title('Data Quality Summary', fontsize=14)\n",
        "\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.suptitle(f'Data Quality Check for {PRIMARY_STOCK}', fontsize=20, fontweight='bold', y=0.98) # Main title\n",
        "    plt.show()\n",
        "\n",
        "    if data_quality_passed:\n",
        "        print(\"\\n Quality Check PASSED!\")\n",
        "        print(\" Data is ready for preprocessing.\")\n",
        "        print(\" Run Data Preprocessing.\")\n",
        "    else:\n",
        "        print(\"\\n  Quality Check FAILED!\")\n",
        "        print(\" Data quality issues detected. Please review the report above.\")\n",
        "        if PROCEED_ON_QUALITY_ISSUES:\n",
        "            print(\"AUTOMATICALLY PROCEEDING due to 'PROCEED_ON_QUALITY_ISSUES' flag being True.\")\n",
        "            print(\" Run  Data Preprocessing.\")\n",
        "        else:\n",
        "            print(\"Execution stopped. Please address data quality issues or set 'PROCEED_ON_QUALITY_ISSUES = True' to continue.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\" No 'stock_data' available. Please ensure Step 1 was executed successfully.\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"  Data Quality Check Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdbHbqJ0G4yo"
      },
      "outputs": [],
      "source": [
        "print(\" Data Preprocessing\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Import TA-Lib (optional, if not installed globally)\n",
        "try:\n",
        "    import talib as ta\n",
        "    TALIB_AVAILABLE = True\n",
        "    print(\" TA-Lib imported successfully.\")\n",
        "except ImportError:\n",
        "    TALIB_AVAILABLE = False\n",
        "    print(\" TA-Lib not available. Using manual indicator calculations.\")\n",
        "\n",
        "def clean_and_normalize_data(data):\n",
        "    \"\"\"\n",
        "    Clean and normalize the stock data.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Raw stock data DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - cleaned_data: Cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    print(\" Cleaning and normalizing data\")\n",
        "\n",
        "    cleaned_data = data.copy()\n",
        "\n",
        "    cleaned_data = cleaned_data[~cleaned_data.index.duplicated(keep='first')]\n",
        "    print(f\"   Removed {len(data) - len(cleaned_data)} duplicate entries.\")\n",
        "\n",
        "    missing_before = cleaned_data.isnull().sum().sum()\n",
        "    cleaned_data = cleaned_data.fillna(method='ffill')\n",
        "    cleaned_data = cleaned_data.fillna(method='bfill')\n",
        "    missing_after = cleaned_data.isnull().sum().sum()\n",
        "    print(f\"   Fixed {missing_before - missing_after} missing values.\")\n",
        "\n",
        "    cleaned_data = cleaned_data.sort_index()\n",
        "\n",
        "    print(\" Data cleaning completed.\")\n",
        "    return cleaned_data\n",
        "\n",
        "def calculate_technical_indicators(data):\n",
        "    \"\"\"\n",
        "    Calculate technical indicators like MACD, RSI, Moving Averages.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Cleaned stock data DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - data_with_indicators: DataFrame with technical indicators.\n",
        "    \"\"\"\n",
        "    print(\" Calculating technical indicators\")\n",
        "\n",
        "    df = data.copy()\n",
        "    close_prices = df['Close'].values\n",
        "    high_prices = df['High'].values\n",
        "    low_prices = df['Low'].values\n",
        "    volume = df['Volume'].values\n",
        "\n",
        "    if TALIB_AVAILABLE:\n",
        "        print(\"   Using TA-Lib for calculations\")\n",
        "        df['SMA_10'] = ta.SMA(close_prices, timeperiod=10)\n",
        "        df['SMA_20'] = ta.SMA(close_prices, timeperiod=20)\n",
        "        df['SMA_50'] = ta.SMA(close_prices, timeperiod=50)\n",
        "        df['EMA_12'] = ta.EMA(close_prices, timeperiod=12)\n",
        "        df['EMA_26'] = ta.EMA(close_prices, timeperiod=26)\n",
        "        df['MACD'], df['MACD_Signal'], df['MACD_Histogram'] = ta.MACD(close_prices)\n",
        "        df['RSI'] = ta.RSI(close_prices, timeperiod=14)\n",
        "        df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = ta.BBANDS(close_prices)\n",
        "        df['Stoch_K'], df['Stoch_D'] = ta.STOCH(high_prices, low_prices, close_prices)\n",
        "        df['Williams_R'] = ta.WILLR(high_prices, low_prices, close_prices)\n",
        "    else:\n",
        "        print(\"   Using manual calculations\")\n",
        "        df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
        "        df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
        "        df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "        df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "        df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "        df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
        "        df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "        df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "        delta = df['Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        df['RSI'] = 100 - (100 / (1 + rs))\n",
        "        df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
        "        bb_std = df['Close'].rolling(window=20).std()\n",
        "        df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
        "        df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
        "\n",
        "    print(\"   Calculating custom indicators\")\n",
        "    df['High_Low_Ratio'] = df['High'] / df['Low']\n",
        "    df['Close_Open_Ratio'] = df['Close'] / df['Open']\n",
        "    df['Price_Change'] = df['Close'].diff()\n",
        "    df['Price_Change_Pct'] = df['Close'].pct_change() * 100\n",
        "    df['Volume_SMA_10'] = df['Volume'].rolling(window=10).mean()\n",
        "\n",
        "    df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA_10'].replace(0, np.nan)\n",
        "    df['Volatility'] = (df['High'] - df['Low']) / df['Close']\n",
        "    df['Volatility_SMA'] = df['Volatility'].rolling(window=10).mean()\n",
        "    df['Price_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low']).replace(0, np.nan) # division by zero\n",
        "    df['Gap'] = df['Open'] - df['Close'].shift(1)\n",
        "    df['Gap_Pct'] = (df['Gap'] / df['Close'].shift(1)).replace(0, np.nan) * 100 # division by zero\n",
        "\n",
        "    print(f\" Calculated {len([col for col in df.columns if col not in data.columns])} technical indicators.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_lagged_features(data, lag_periods=[1, 2, 3, 5, 10]):\n",
        "    \"\"\"\n",
        "    Create lagged features for time series analysis.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame with technical indicators.\n",
        "    - lag_periods: List of lag periods to create.\n",
        "\n",
        "    Returns:\n",
        "    - data_with_lags: DataFrame with lagged features.\n",
        "    \"\"\"\n",
        "    print(\" Creating lagged features\")\n",
        "\n",
        "    df = data.copy()\n",
        "    key_features = ['Close', 'Volume', 'RSI', 'MACD', 'Price_Change_Pct', 'Volatility']\n",
        "\n",
        "    for feature in key_features:\n",
        "        if feature in df.columns:\n",
        "            for lag in lag_periods:\n",
        "                df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
        "\n",
        "    print(f\" Created lagged features for periods: {lag_periods}.\")\n",
        "    return df\n",
        "\n",
        "def prepare_features_for_ml(data, target_column='Close', time_steps=30):\n",
        "    \"\"\"\n",
        "    Prepare features and target for machine learning models, including scaling.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame with all features.\n",
        "    - target_column: Name of target variable.\n",
        "    - time_steps: Number of time steps for sequence data (for LSTM/CNN).\n",
        "\n",
        "    Returns:\n",
        "    - X_scaled: Feature matrix (scaled) for traditional ML.\n",
        "    - y_scaled: Target vector (scaled) for traditional ML.\n",
        "    - feature_names: List of feature names.\n",
        "    - feature_scaler: Fitted StandardScaler object for features.\n",
        "    - scaler_y: Fitted MinMaxScaler object for target.\n",
        "    - final_data_processed: Final cleaned DataFrame (after dropping NaNs).\n",
        "    \"\"\"\n",
        "    print(\" Preparing features for machine learning\")\n",
        "\n",
        "    df_clean = data.dropna()\n",
        "    print(f\"   Removed {len(data) - len(df_clean)} rows with NaN values (from indicators/lags).\")\n",
        "    print(f\"   Final dataset size after cleaning: {len(df_clean)} rows.\")\n",
        "\n",
        "    exclude_columns = [target_column, 'Open', 'High', 'Low', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
        "    feature_columns = [col for col in df_clean.columns if col not in exclude_columns and 'Unnamed' not in col]\n",
        "\n",
        "    if not feature_columns:\n",
        "        raise ValueError(\"No valid features remaining after exclusion. Check exclude_columns or data.\")\n",
        "\n",
        "    X = df_clean[feature_columns]\n",
        "    y = df_clean[target_column]\n",
        "\n",
        "    print(f\"   Selected {len(feature_columns)} features.\")\n",
        "    print(f\"   Sample feature columns: {feature_columns[:5]}...{feature_columns[-5:] if len(feature_columns) > 5 else ''}.\")\n",
        "\n",
        "    # Scale features\n",
        "    feature_scaler = StandardScaler()\n",
        "    X_scaled = feature_scaler.fit_transform(X)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=feature_columns, index=df_clean.index)\n",
        "    print(\"    Features scaled using StandardScaler.\")\n",
        "\n",
        "    scaler_y = MinMaxScaler()\n",
        "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "    y_scaled = pd.Series(y_scaled, index=y.index)\n",
        "    print(\"  Target variable 'y' scaled using MinMaxScaler.\")\n",
        "\n",
        "\n",
        "    print(\" Features and target prepared and scaled.\")\n",
        "\n",
        "    return X_scaled, y_scaled, feature_columns, feature_scaler, scaler_y, df_clean\n",
        "\n",
        "#  PREPROCESSING PIPELINE\n",
        "\n",
        "if 'stock_data' in locals() and stock_data is not None:\n",
        "    print(\"\\n Starting preprocessing pipeline\")\n",
        "\n",
        "    cleaned_data = clean_and_normalize_data(stock_data)\n",
        "\n",
        "    data_with_indicators = calculate_technical_indicators(cleaned_data)\n",
        "\n",
        "    data_with_lags = create_lagged_features(data_with_indicators)\n",
        "\n",
        "    X, y, feature_names, feature_scaler, scaler_y, final_data = \\\n",
        "        prepare_features_for_ml(data_with_lags, target_column='Close')\n",
        "\n",
        "    print(\"\\n Preprocessing Results:\")\n",
        "    print(f\"   Original data shape: {stock_data.shape}\")\n",
        "    print(f\"   Final processed data shape (after NaN removal): {final_data.shape}\")\n",
        "    print(f\"   Features (X) shape (scaled): {X.shape}\")\n",
        "    print(f\"   Target (y) shape (scaled): {y.shape}\")\n",
        "    print(f\"   Number of features: {len(feature_names)}\")\n",
        "\n",
        "    print(\"\\n Sample of processed features (scaled):\")\n",
        "    print(X.head())\n",
        "\n",
        "    print(\"\\n Sample of target values (scaled):\")\n",
        "    print(y.head())\n",
        "\n",
        " # Visualization Preprocessing Results\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    #  Original vs processed price data\n",
        "    plt.subplot(3, 4, 1)\n",
        "    if 'Close' in final_data.columns and 'SMA_20' in final_data.columns:\n",
        "        plt.plot(final_data.index, final_data['Close'], label='Original Close Price', linewidth=1.5)\n",
        "        plt.plot(final_data.index, final_data['SMA_20'], label='SMA 20', alpha=0.7, linestyle='--')\n",
        "        plt.title('Price with Moving Average', fontsize=12)\n",
        "        plt.xlabel('Date', fontsize=10)\n",
        "        plt.ylabel('Price', fontsize=10)\n",
        "        plt.legend(fontsize=9)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Close/SMA_20 not available.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('Price with Moving Average (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "    # MACD\n",
        "    plt.subplot(3, 4, 2)\n",
        "    if 'MACD' in final_data.columns and 'MACD_Signal' in final_data.columns:\n",
        "        plt.plot(final_data.index, final_data['MACD'], label='MACD', linewidth=1)\n",
        "        plt.plot(final_data.index, final_data['MACD_Signal'], label='Signal', linestyle='--', linewidth=1)\n",
        "        plt.bar(final_data.index, final_data['MACD_Histogram'], label='Histogram', color='gray', alpha=0.5)\n",
        "        plt.title('MACD Indicator', fontsize=12)\n",
        "        plt.xlabel('Date', fontsize=10)\n",
        "        plt.ylabel('Value', fontsize=10)\n",
        "        plt.legend(fontsize=9)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'MACD not available.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('MACD Indicator (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "\n",
        "    # RSI\n",
        "    plt.subplot(3, 4, 3)\n",
        "    if 'RSI' in final_data.columns:\n",
        "        plt.plot(final_data.index, final_data['RSI'], linewidth=1.5, color='purple')\n",
        "        plt.axhline(y=70, color='r', linestyle='--', alpha=0.6, label='Overbought (70)')\n",
        "        plt.axhline(y=30, color='g', linestyle='--', alpha=0.6, label='Oversold (30)')\n",
        "        plt.title('RSI Indicator', fontsize=12)\n",
        "        plt.xlabel('Date', fontsize=10)\n",
        "        plt.ylabel('RSI', fontsize=10)\n",
        "        plt.legend(fontsize=9)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'RSI not available.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('RSI Indicator (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Bollinger Bands\n",
        "    plt.subplot(3, 4, 4)\n",
        "    if all(col in final_data.columns for col in ['Close', 'BB_Upper', 'BB_Lower']):\n",
        "        plt.plot(final_data.index, final_data['Close'], label='Close', linewidth=1.5)\n",
        "        plt.plot(final_data.index, final_data['BB_Upper'], label='Upper Band', alpha=0.7, linestyle=':')\n",
        "        plt.plot(final_data.index, final_data['BB_Lower'], label='Lower Band', alpha=0.7, linestyle=':')\n",
        "        plt.fill_between(final_data.index, final_data['BB_Upper'], final_data['BB_Lower'], color='skyblue', alpha=0.1)\n",
        "        plt.title('Bollinger Bands', fontsize=12)\n",
        "        plt.xlabel('Date', fontsize=10)\n",
        "        plt.ylabel('Price', fontsize=10)\n",
        "        plt.legend(fontsize=9)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Bollinger Bands not available.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('Bollinger Bands (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "    #Volume analysis\n",
        "    plt.subplot(3, 4, 5)\n",
        "    if 'Volume' in final_data.columns and 'Volume_SMA_10' in final_data.columns:\n",
        "        plt.bar(final_data.index, final_data['Volume'], label='Volume', color='lightgreen', alpha=0.7)\n",
        "        plt.plot(final_data.index, final_data['Volume_SMA_10'], label='Volume SMA', color='darkgreen', linestyle='--')\n",
        "        plt.title('Volume Analysis', fontsize=12)\n",
        "        plt.xlabel('Date', fontsize=10)\n",
        "        plt.ylabel('Volume', fontsize=10)\n",
        "        plt.legend(fontsize=9)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Volume/Volume SMA not available.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('Volume Analysis (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "\n",
        "    # Price changes\n",
        "    plt.subplot(3, 4, 6)\n",
        "    if 'Price_Change_Pct' in final_data.columns:\n",
        "        plt.plot(final_data.index, final_data['Price_Change_Pct'], color='salmon', linewidth=1)\n",
        "        plt.title('Daily Price Changes (%)', fontsize=12)\n",
        "        plt.xlabel('Date', fontsize=10)\n",
        "        plt.ylabel('Change %', fontsize=10)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Price_Change_Pct not available.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('Daily Price Changes (%) (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Volatility\n",
        "    plt.subplot(3, 4, 7)\n",
        "    if 'Volatility' in final_data.columns and 'Volatility_SMA' in final_data.columns:\n",
        "        plt.plot(final_data.index, final_data['Volatility'], label='Daily Volatility', color='orange', linewidth=1)\n",
        "        plt.plot(final_data.index, final_data['Volatility_SMA'], label='Volatility SMA', color='darkred', linestyle='--')\n",
        "        plt.title('Volatility Analysis', fontsize=12)\n",
        "        plt.xlabel('Date', fontsize=10)\n",
        "        plt.ylabel('Volatility', fontsize=10)\n",
        "        plt.legend(fontsize=9)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Volatility not available.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('Volatility Analysis (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "    #  Feature correlation heatmap\n",
        "    plt.subplot(3, 4, 8)\n",
        "\n",
        "    plot_features = ['Close', 'SMA_20', 'RSI', 'MACD', 'Volume_Ratio', 'Volatility', 'Price_Change_Pct']\n",
        "    available_plot_features = [f for f in plot_features if f in final_data.columns]\n",
        "    if len(available_plot_features) >= 2: # Need at least 2 features for a correlation matrix\n",
        "        corr_matrix = final_data[available_plot_features].corr()\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5, cbar_kws={\"shrink\": .7})\n",
        "        plt.title('Feature Correlation', fontsize=12)\n",
        "        plt.xticks(fontsize=8, rotation=45, ha='right')\n",
        "        plt.yticks(fontsize=8, rotation=0)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Not enough features for correlation plot.', ha='center', va='center', fontsize=10)\n",
        "        plt.title('Feature Correlation (N/A)', fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "\n",
        "    # Feature distributions for selected features\n",
        "    feature_cols_for_dist = ['RSI', 'MACD', 'Price_Change_Pct', 'Volatility']\n",
        "    for i, col in enumerate(feature_cols_for_dist, 9):\n",
        "        plt.subplot(3, 4, i)\n",
        "        if col in final_data.columns:\n",
        "            plt.hist(final_data[col].dropna(), bins=30, alpha=0.7, color='teal', edgecolor='black')\n",
        "            plt.title(f'{col} Distribution', fontsize=12)\n",
        "            plt.xlabel(col, fontsize=10)\n",
        "            plt.ylabel('Frequency', fontsize=10)\n",
        "            plt.xticks(fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "            plt.grid(axis='y', linestyle=':', alpha=0.6)\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, f'{col} not found.', ha='center', va='center', fontsize=10)\n",
        "            plt.title(f'{col} Distribution (N/A)', fontsize=12)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f'Data Preprocessing & Feature Engineering Visualizations for {PRIMARY_STOCK}', fontsize=20, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n Data Preprocessing Successful!\")\n",
        "    print(\" Data is ready for model training.\")\n",
        "else:\n",
        "    print(\" No 'stock_data' available. Please run Steps 1 and 2 first.\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\" Preprocessing  Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGUfDkIPHIZr"
      },
      "outputs": [],
      "source": [
        "print(\" Model Selection and Training\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# DATA PREPARATION FOR  MODELS (LSTM/CNN)\n",
        "def prepare_time_series_data(X, y, time_steps=30):\n",
        "    \"\"\"\n",
        "    Prepare time series data for LSTM/CNN models.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Feature matrix (pandas DataFrame or numpy array).\n",
        "    - y: Target vector (pandas Series or numpy array).\n",
        "    - time_steps: Number of time steps to use for each sample.\n",
        "\n",
        "    Returns:\n",
        "    - X_series: Reshaped feature array for time series models (3D).\n",
        "    - y_series: Corresponding target values.\n",
        "    \"\"\"\n",
        "    X_series, y_series = [], []\n",
        "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
        "    y_np = y.values if isinstance(y, pd.Series) else y\n",
        "\n",
        "    for i in range(time_steps, len(X_np)):\n",
        "        X_series.append(X_np[i-time_steps:i])\n",
        "        y_series.append(y_np[i])\n",
        "\n",
        "    X_series = np.array(X_series)\n",
        "    y_series = np.array(y_series)\n",
        "\n",
        "    print(f\"   Time series data prepared: X_series shape {X_series.shape}, y_series shape {y_series.shape}.\")\n",
        "    return X_series, y_series\n",
        "\n",
        "# DEEP LEARNING MODEL ARCHITECTURES\n",
        "def build_lstm_model(input_shape):\n",
        "    \"\"\"\n",
        "    Build and compile an LSTM model.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: Shape of input data (timesteps, features).\n",
        "\n",
        "    Returns:\n",
        "    - model: Compiled LSTM model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=input_shape, activation='relu', kernel_initializer='he_normal'),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, return_sequences=False, activation='relu', kernel_initializer='he_normal'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    print(\"   LSTM model built and compiled.\")\n",
        "    return model\n",
        "\n",
        "def build_cnn_model(input_shape):\n",
        "    \"\"\"\n",
        "    Build and compile a CNN model for time series.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: Shape of input data (timesteps, features).\n",
        "\n",
        "    Returns:\n",
        "    - model: Compiled CNN model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, kernel_initializer='he_normal'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu', kernel_initializer='he_normal'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(50, activation='relu', kernel_initializer='he_normal'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    print(\"   CNN model built and compiled.\")\n",
        "    return model\n",
        "\n",
        "# MODEL TRAINING AND EVALUATION FUNCTION\n",
        "def train_evaluate_models(X, y, primary_stock):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple models (XGBoost, SVM, LSTM, CNN).\n",
        "\n",
        "    Parameters:\n",
        "    - X: Feature matrix (DataFrame).\n",
        "    - y: Target vector (Series).\n",
        "    - primary_stock: Stock symbol for reporting.\n",
        "\n",
        "    Returns:\n",
        "    - results: Dictionary with model performance metrics.\n",
        "    - models: Dictionary with trained models.\n",
        "    - X_train, X_test, y_train, y_test: Split data for traditional ML models (global scope).\n",
        "    - X_train_series, X_test_series, y_train_series, y_test_series: Split data for DL models (global scope).\n",
        "    \"\"\"\n",
        "    print(f\"\\n Training models for {primary_stock}\")\n",
        "\n",
        "    # 1. Prepare data for traditional ML models (Standard Split)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, shuffle=False, random_state=42\n",
        "    )\n",
        "    print(f\"   Traditional ML data splits: X_train:{X_train.shape}, X_test:{X_test.shape}.\")\n",
        "\n",
        "\n",
        "\n",
        "    global time_steps\n",
        "    time_steps = 30\n",
        "\n",
        "    X_series, y_series = prepare_time_series_data(X, y, time_steps)\n",
        "\n",
        "\n",
        "    split_idx_series = int(0.8 * len(X_series)) # 80% for training, 20% for testing\n",
        "\n",
        "    X_train_series, X_test_series = X_series[:split_idx_series], X_series[split_idx_series:]\n",
        "    y_train_series, y_test_series = y_series[:split_idx_series], y_series[split_idx_series:]\n",
        "    print(f\"   Time Series data splits: X_train_series:{X_train_series.shape}, X_test_series:{X_test_series.shape}.\")\n",
        "\n",
        "\n",
        "    results = {}\n",
        "    models = {}\n",
        "\n",
        "    # 1. Train and evaluate XGBoost\n",
        "    print(\"\\n Training XGBoost model...\")\n",
        "\n",
        "    xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5,\n",
        "                             subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1,\n",
        "                             tree_method='hist',\n",
        "                             eval_metric='rmse',\n",
        "                             objective='reg:squarederror'\n",
        "                            )\n",
        "\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_pred = xgb_model.predict(X_test)\n",
        "    xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
        "    xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
        "    xgb_r2 = r2_score(y_test, xgb_pred)\n",
        "\n",
        "    results['XGBoost'] = {'MSE': xgb_mse, 'MAE': xgb_mae, 'R2': xgb_r2}\n",
        "    models['XGBoost'] = xgb_model\n",
        "    print(f\" XGBoost trained - MSE: {xgb_mse:.4f}, MAE: {xgb_mae:.4f}, R2: {xgb_r2:.4f}\")\n",
        "\n",
        "    # 2. Train and evaluate SVM\n",
        "    print(\"\\n Training SVM model...\")\n",
        "\n",
        "    svm_model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "    svm_model.fit(X_train, y_train)\n",
        "\n",
        "    svm_pred = svm_model.predict(X_test)\n",
        "    svm_mse = mean_squared_error(y_test, svm_pred)\n",
        "    svm_mae = mean_absolute_error(y_test, svm_pred)\n",
        "    svm_r2 = r2_score(y_test, svm_pred)\n",
        "\n",
        "    results['SVM'] = {'MSE': svm_mse, 'MAE': svm_mae, 'R2': svm_r2}\n",
        "    models['SVM'] = svm_model\n",
        "    print(f\" SVM trained - MSE: {svm_mse:.4f}, MAE: {svm_mae:.4f}, R2: {svm_r2:.4f}\")\n",
        "\n",
        "    # 3. Train and evaluate LSTM\n",
        "\n",
        "    print(\"\\n Training LSTM model...\")\n",
        "    lstm_model = build_lstm_model((X_train_series.shape[1], X_train_series.shape[2]))\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    # Model checkpoint to save the best model during training\n",
        "    # checkpoint_filepath_lstm = f'{primary_stock}_best_lstm_model.h5'\n",
        "    # model_checkpoint_callback_lstm = ModelCheckpoint(\n",
        "    #     filepath=checkpoint_filepath_lstm,\n",
        "    #     save_best_only=True,\n",
        "    #     monitor='val_loss',\n",
        "    #     mode='min',\n",
        "    #     verbose=0\n",
        "    # )\n",
        "\n",
        "    history_lstm = lstm_model.fit(\n",
        "        X_train_series, y_train_series,\n",
        "        validation_data=(X_test_series, y_test_series),\n",
        "        epochs=150,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    lstm_pred = lstm_model.predict(X_test_series).flatten()\n",
        "    lstm_mse = mean_squared_error(y_test_series, lstm_pred)\n",
        "    lstm_mae = mean_absolute_error(y_test_series, lstm_pred)\n",
        "    lstm_r2 = r2_score(y_test_series, lstm_pred)\n",
        "\n",
        "    results['LSTM'] = {'MSE': lstm_mse, 'MAE': lstm_mae, 'R2': lstm_r2, 'history': history_lstm.history}\n",
        "    models['LSTM'] = lstm_model\n",
        "    print(f\" LSTM trained - MSE: {lstm_mse:.4f}, MAE: {lstm_mae:.4f}, R2: {lstm_r2:.4f}\")\n",
        "\n",
        "    # 4. Train and evaluate CNN\n",
        "    print(\"\\n Training CNN model...\")\n",
        "    cnn_model = build_cnn_model((X_train_series.shape[1], X_train_series.shape[2]))\n",
        "\n",
        "    early_stopping_cnn = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    # checkpoint_filepath_cnn = f'{primary_stock}_best_cnn_model.h5'\n",
        "    # model_checkpoint_callback_cnn = ModelCheckpoint(\n",
        "    #     filepath=checkpoint_filepath_cnn,\n",
        "    #     save_best_only=True,\n",
        "    #     monitor='val_loss',\n",
        "    #     mode='min',\n",
        "    #     verbose=0\n",
        "    # )\n",
        "\n",
        "    history_cnn = cnn_model.fit(\n",
        "        X_train_series, y_train_series,\n",
        "        validation_data=(X_test_series, y_test_series),\n",
        "        epochs=150,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping_cnn],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    cnn_pred = cnn_model.predict(X_test_series).flatten()\n",
        "    cnn_mse = mean_squared_error(y_test_series, cnn_pred)\n",
        "    cnn_mae = mean_absolute_error(y_test_series, cnn_pred)\n",
        "    cnn_r2 = r2_score(y_test_series, cnn_pred)\n",
        "\n",
        "    results['CNN'] = {'MSE': cnn_mse, 'MAE': cnn_mae, 'R2': cnn_r2, 'history': history_cnn.history}\n",
        "    models['CNN'] = cnn_model\n",
        "    print(f\" CNN trained - MSE: {cnn_mse:.4f}, MAE: {cnn_mae:.4f}, R2: {cnn_r2:.4f}\")\n",
        "\n",
        "    return results, models, X_train, X_test, y_train, y_test, X_train_series, X_test_series, y_train_series, y_test_series\n",
        "\n",
        "def visualize_model_performance(results, primary_stock):\n",
        "    \"\"\"\n",
        "    Visualize model performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - results: Dictionary with model performance metrics.\n",
        "    - primary_stock: Stock symbol for title.\n",
        "    \"\"\"\n",
        "    print(\"\\n Visualizing initial model performance...\")\n",
        "\n",
        "    plt.figure(figsize=(18, 10))\n",
        "\n",
        "    #  Model comparison - MSE\n",
        "    plt.subplot(2, 2, 1)\n",
        "    mse_values = [results[model]['MSE'] for model in results]\n",
        "    model_names = list(results.keys())\n",
        "    plt.bar(model_names, mse_values, color=['blue', 'green', 'red', 'purple'])\n",
        "    plt.title('Model Comparison - Mean Squared Error', fontsize=14)\n",
        "    plt.ylabel('MSE (Lower is better)', fontsize=12)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "\n",
        "    # Model comparison - MAE\n",
        "    plt.subplot(2, 2, 2)\n",
        "    mae_values = [results[model]['MAE'] for model in results]\n",
        "    plt.bar(model_names, mae_values, color=['blue', 'green', 'red', 'purple'])\n",
        "    plt.title('Model Comparison - Mean Absolute Error', fontsize=14)\n",
        "    plt.ylabel('MAE (Lower is better)', fontsize=12)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "\n",
        "    # Model comparison - R2\n",
        "    plt.subplot(2, 2, 3)\n",
        "    r2_values = [results[model]['R2'] for model in results]\n",
        "    plt.bar(model_names, r2_values, color=['blue', 'green', 'red', 'purple'])\n",
        "    plt.title('Model Comparison - R-squared', fontsize=14)\n",
        "    plt.ylabel('R2 (Higher is better)', fontsize=12)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "\n",
        "    # Training history for deep learning models\n",
        "    plt.subplot(2, 2, 4)\n",
        "    for model_name in ['LSTM', 'CNN']:\n",
        "        if model_name in results and 'history' in results[model_name]:\n",
        "            plt.plot(results[model_name]['history']['loss'], label=f'{model_name} Train Loss', linewidth=1.5)\n",
        "            plt.plot(results[model_name]['history']['val_loss'], label=f'{model_name} Validation Loss', linestyle='--', linewidth=1.5)\n",
        "    plt.title('Training History - Loss (Deep Learning Models)', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.suptitle(f'Initial Model Performance Comparison for {primary_stock}', fontsize=18, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
        "    plt.show()\n",
        "\n",
        "# EXECUTE MODEL TRAINING AND EVALUATION (Execution Block)\n",
        "\n",
        "if 'X' in locals() and 'y' in locals() and 'PRIMARY_STOCK' in locals():\n",
        "    print(\"\\n Starting initial model training pipeline...\")\n",
        "\n",
        "    model_results, trained_models, X_train, X_test, y_train, y_test, X_train_series, X_test_series, y_train_series, y_test_series = \\\n",
        "        train_evaluate_models(X, y, PRIMARY_STOCK)\n",
        "\n",
        "    # Visualize initial performance\n",
        "    visualize_model_performance(model_results, PRIMARY_STOCK)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n Initial Model Performance Summary:\")\n",
        "    for model_name, metrics in model_results.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  MSE: {metrics['MSE']:.4f}\")\n",
        "        print(f\"  MAE: {metrics['MAE']:.4f}\")\n",
        "        print(f\"  R2: {metrics['R2']:.4f}\")\n",
        "\n",
        "    print(\"\\n  Initial Model Training and Evaluation Successful!\")\n",
        "else:\n",
        "    print(\" Processed data (X, y) or PRIMARY_STOCK not available. run Steps 1-3 first.\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZP-HVtfH7zg"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Hyperparameter Optimization\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "if 'X_train' in locals() and 'y_train' in locals() and 'X_test' in locals() and 'y_test' in locals() and \\\n",
        "   'X_train_series' in locals() and 'y_train_series' in locals() and 'X_test_series' in locals() and 'y_test_series' in locals() and \\\n",
        "   'time_steps' in locals() and 'feature_names' in locals():\n",
        "    print(\" All required data (X_train, y_train, etc.) available for hyperparameter optimization.\")\n",
        "\n",
        "    def build_lstm_model_for_tuning(meta, neurons=64, dropout_rate=0.2, learning_rate=0.001):\n",
        "        \"\"\"Builds an LSTM model for hyperparameter tuning.\"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(neurons, return_sequences=True, input_shape=(time_steps, X_train_series.shape[2]), activation='relu'),\n",
        "            Dropout(dropout_rate),\n",
        "            LSTM(neurons // 2, return_sequences=False, activation='relu'),\n",
        "            Dropout(dropout_rate),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "\n",
        "    def build_cnn_model_for_tuning(meta, filters1=64, filters2=32, kernel_size1=3, dense_units=50, dropout_rate=0.2, learning_rate=0.001):\n",
        "        \"\"\"Builds a CNN model for hyperparameter tuning.\"\"\"\n",
        "        model = Sequential([\n",
        "            Conv1D(filters=filters1, kernel_size=kernel_size1, activation='relu', input_shape=(time_steps, X_train_series.shape[2])),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Conv1D(filters=filters2, kernel_size=kernel_size1, activation='relu'),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Flatten(),\n",
        "            Dense(dense_units, activation='relu'),\n",
        "            Dropout(dropout_rate),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "\n",
        "    # Hyperparameter Tuning for Traditional ML Models (XGBoost, SVM)\n",
        "\n",
        "    print(\"\\n Tuning Hyperparameters for Traditional ML Models (XGBoost, SVM)\")\n",
        "\n",
        "    X_train_np = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
        "    y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
        "    X_test_np = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
        "    y_test_np = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
        "\n",
        "    # XGBoost Hyperparameter Tuning\n",
        "    print(\"  - Tuning XGBoost (this might take a few minutes)\")\n",
        "    param_grid_xgb = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.05, 0.1, 0.15],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.7, 0.9],\n",
        "        'colsample_bytree': [0.7, 0.9]\n",
        "    }\n",
        "    xgb_model_base = XGBRegressor(random_state=42, eval_metric='rmse', objective='reg:squarederror')\n",
        "    grid_search_xgb = GridSearchCV(xgb_model_base, param_grid_xgb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=0)\n",
        "    grid_search_xgb.fit(X_train_np, y_train_np)\n",
        "\n",
        "    best_xgb_model = grid_search_xgb.best_estimator_\n",
        "    print(f\"  Best XGBoost Parameters: {grid_search_xgb.best_params_}\")\n",
        "    print(f\"  Best XGBoost Validation MSE: {-grid_search_xgb.best_score_:.4f}\")\n",
        "\n",
        "    # SVM Hyperparameter Tuning\n",
        "    print(\"  - Tuning SVM (this might take a while, especially on larger datasets)...\")\n",
        "    param_grid_svm = {\n",
        "        'C': [10, 100],\n",
        "        'gamma': [0.01, 0.1],\n",
        "        'kernel': ['rbf']\n",
        "    }\n",
        "    svm_model_base = SVR()\n",
        "    grid_search_svm = GridSearchCV(svm_model_base, param_grid_svm, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=0)\n",
        "    grid_search_svm.fit(X_train_np, y_train_np)\n",
        "\n",
        "    best_svm_model = grid_search_svm.best_estimator_\n",
        "    print(f\"  Best SVM Parameters: {grid_search_svm.best_params_}\")\n",
        "    print(f\"  Best SVM Validation MSE: {-grid_search_svm.best_score_:.4f}\")\n",
        "\n",
        "    # Hyperparameter Tuning for Deep Learning Models (LSTM, CNN)\n",
        "\n",
        "    print(\"\\n Tuning Hyperparameters for Deep Learning Models (LSTM, CNN) (time consuming) \")\n",
        "\n",
        "    print(\"  - Tuning LSTM (takes a while)...\")\n",
        "    lstm_model_base = KerasRegressor(\n",
        "        model=build_lstm_model_for_tuning,\n",
        "        verbose=0 # Suppress verbose output during tuning\n",
        "    )\n",
        "\n",
        "    param_dist_lstm = {\n",
        "        'model__neurons': [32, 48, 64],\n",
        "        'model__dropout_rate': [0.1, 0.2, 0.3],\n",
        "        'model__learning_rate': [0.001, 0.0005, 0.0001],\n",
        "        'batch_size': [16, 32, 64],\n",
        "        'epochs': [30, 40]\n",
        "    }\n",
        "\n",
        "    random_search_lstm = RandomizedSearchCV(\n",
        "        estimator=lstm_model_base,\n",
        "        param_distributions=param_dist_lstm,\n",
        "        n_iter=10,\n",
        "        cv=3,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=1,\n",
        "        verbose=0,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    random_search_lstm.fit(\n",
        "        X_train_series, y_train_series,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)]\n",
        "    )\n",
        "\n",
        "    best_lstm_model_wrapper = random_search_lstm.best_estimator_\n",
        "    best_lstm_params = random_search_lstm.best_params_\n",
        "    print(f\"  Best LSTM Parameters: {best_lstm_params}\")\n",
        "    print(f\"  Best LSTM Validation MSE: {-random_search_lstm.best_score_:.4f}\")\n",
        "\n",
        "    # CNN Hyperparameter Tuning\n",
        "    print(\"  - Tuning CNN (takes a while)...\")\n",
        "    cnn_model_base = KerasRegressor(\n",
        "        model=build_cnn_model_for_tuning,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    param_dist_cnn = {\n",
        "        'model__filters1': [32, 64, 96],\n",
        "        'model__filters2': [16, 32, 48],\n",
        "        'model__kernel_size1': [2, 3, 4],\n",
        "        'model__dense_units': [30, 50, 70],\n",
        "        'model__dropout_rate': [0.1, 0.2, 0.3],\n",
        "        'model__learning_rate': [0.001, 0.0005, 0.0001],\n",
        "        'batch_size': [16, 32, 64],\n",
        "        'epochs': [30, 40]\n",
        "    }\n",
        "\n",
        "    random_search_cnn = RandomizedSearchCV(\n",
        "        estimator=cnn_model_base,\n",
        "        param_distributions=param_dist_cnn,\n",
        "        n_iter=10,\n",
        "        cv=3,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=1,\n",
        "        verbose=0,\n",
        "        random_state=42\n",
        "    )\n",
        "    random_search_cnn.fit(\n",
        "        X_train_series, y_train_series,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)]\n",
        "    )\n",
        "\n",
        "    best_cnn_model_wrapper = random_search_cnn.best_estimator_\n",
        "    best_cnn_params = random_search_cnn.best_params_\n",
        "    print(f\"  Best CNN Parameters: {best_cnn_params}\")\n",
        "    print(f\"  Best CNN Validation MSE: {-random_search_cnn.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "    # Evaluate All Optimized Models on the Test Set and Update Global Results\n",
        "\n",
        "    print(\"\\n Evaluating Optimized Models on the Global Test Set\")\n",
        "    optimized_model_results = {}\n",
        "    optimized_trained_models = {}\n",
        "\n",
        "    # Traditional models\n",
        "    for name, model in {'XGBoost': best_xgb_model, 'SVM': best_svm_model}.items():\n",
        "        y_pred = model.predict(X_test_np)\n",
        "        mse = mean_squared_error(y_test_np, y_pred)\n",
        "        mae = mean_absolute_error(y_test_np, y_pred)\n",
        "        r2 = r2_score(y_test_np, y_pred)\n",
        "        optimized_model_results[name] = {'MSE': mse, 'MAE': mae, 'R2': r2}\n",
        "        optimized_trained_models[name] = model\n",
        "        print(f\"  Optimized {name} - MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
        "\n",
        "    # Deep Learning models\n",
        "\n",
        "    print(\"\\n  Evaluating Optimized Deep Learning Models\")\n",
        "    # LSTM\n",
        "    lstm_opt_pred = best_lstm_model_wrapper.predict(X_test_series).flatten()\n",
        "    lstm_opt_mse = mean_squared_error(y_test_series, lstm_opt_pred)\n",
        "    lstm_opt_mae = mean_absolute_error(y_test_series, lstm_opt_pred)\n",
        "    lstm_opt_r2 = r2_score(y_test_series, lstm_opt_pred)\n",
        "    optimized_model_results['LSTM'] = {'MSE': lstm_opt_mse, 'MAE': lstm_opt_mae, 'R2': lstm_opt_r2}\n",
        "    optimized_trained_models['LSTM'] = best_lstm_model_wrapper\n",
        "\n",
        "    print(f\"  Optimized LSTM - MSE: {lstm_opt_mse:.4f}, MAE: {lstm_opt_mae:.4f}, R2: {lstm_opt_r2:.4f}\")\n",
        "\n",
        "    # CNN\n",
        "    cnn_opt_pred = best_cnn_model_wrapper.predict(X_test_series).flatten()\n",
        "    cnn_opt_mse = mean_squared_error(y_test_series, cnn_opt_pred)\n",
        "    cnn_opt_mae = mean_absolute_error(y_test_series, cnn_opt_pred)\n",
        "    cnn_opt_r2 = r2_score(y_test_series, cnn_opt_pred)\n",
        "    optimized_model_results['CNN'] = {'MSE': cnn_opt_mse, 'MAE': cnn_opt_mae, 'R2': cnn_opt_r2}\n",
        "    optimized_trained_models['CNN'] = best_cnn_model_wrapper\n",
        "\n",
        "    print(f\"  Optimized CNN - MSE: {cnn_opt_mse:.4f}, MAE: {cnn_opt_mae:.4f}, R2: {cnn_opt_r2:.4f}\")\n",
        "\n",
        "    # Update global variables with optimized results\n",
        "\n",
        "    model_results = optimized_model_results\n",
        "    trained_models = optimized_trained_models\n",
        "    print(\"\\n Global 'model_results' and 'trained_models' updated with optimized models.\")\n",
        "\n",
        "else:\n",
        "    print(\" Required data (X_train, y_train, etc.) not found. Please ensure Steps 1-4 are completed and data is prepared.\")\n",
        "\n",
        "print(\"\\n  Hyperparameter Optimization Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Model Evaluation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if 'model_results' in locals() and 'trained_models' in locals() and \\\n",
        "   'X_test' in locals() and 'y_test' in locals() and \\\n",
        "   'X_test_series' in locals() and 'y_test_series' in locals() and \\\n",
        "   'PRIMARY_STOCK' in locals() and 'scaler_y' in locals() and scaler_y is not None and \\\n",
        "   'final_data' in locals():\n",
        "    print(\" Optimized model results and trained models available for comprehensive evaluation.\")\n",
        "\n",
        "    global BEST_MODEL, best_model_name, best_r2_score, y_test_rescaled, y_pred_best_rescaled # Make these globally available\n",
        "\n",
        "    best_model_name = None\n",
        "    best_r2_score = -float('inf')\n",
        "    best_mse = float('inf')\n",
        "\n",
        "    print(\"\\n Comprehensive Model Performance Summary (Optimized Models):\")\n",
        "    for model_name, metrics in model_results.items():\n",
        "        print(f\"\\n--- {model_name} ---\")\n",
        "        print(f\"  MSE: {metrics['MSE']:.4f}\")\n",
        "        print(f\"  MAE: {metrics['MAE']:.4f}\")\n",
        "        print(f\"  R2: {metrics['R2']:.4f}\")\n",
        "\n",
        "\n",
        "        if metrics['R2'] > best_r2_score:\n",
        "            best_r2_score = metrics['R2']\n",
        "            best_mse = metrics['MSE']\n",
        "            best_model_name = model_name\n",
        "        elif metrics['R2'] == best_r2_score and metrics['MSE'] < best_mse:\n",
        "            best_mse = metrics['MSE']\n",
        "            best_model_name = model_name\n",
        "\n",
        "    if best_model_name:\n",
        "        print(f\"\\n Overall Best Performing Model: {best_model_name} with R2 Score: {best_r2_score:.4f} and MSE: {best_mse:.4f}.\")\n",
        "        BEST_MODEL = trained_models[best_model_name]\n",
        "    else:\n",
        "        print(\" No optimized models found in 'model_results' for comparison.\")\n",
        "        BEST_MODEL = None\n",
        "\n",
        "    # Visualize Best Model's Predictions vs. Actual Prices\n",
        "\n",
        "    if BEST_MODEL is not None:\n",
        "        print(\"\\n Visualizing Best Model's Predictions on the test set:\")\n",
        "        try:\n",
        "\n",
        "            if best_model_name in ['LSTM', 'CNN']:\n",
        "                test_features = X_test_series\n",
        "                actual_prices_scaled = y_test_series\n",
        "                print(f\"  Using time series data (3D) for {best_model_name} prediction.\")\n",
        "            else:\n",
        "                test_features = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
        "                actual_prices_scaled = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
        "                print(f\"  Using traditional ML data (2D) for {best_model_name} prediction.\")\n",
        "\n",
        "\n",
        "            y_pred_best_scaled = BEST_MODEL.predict(test_features)\n",
        "\n",
        "\n",
        "            if isinstance(y_pred_best_scaled, np.ndarray) and y_pred_best_scaled.ndim > 1:\n",
        "                y_pred_best_scaled = y_pred_best_scaled.flatten()\n",
        "\n",
        "\n",
        "            y_test_rescaled = scaler_y.inverse_transform(actual_prices_scaled.reshape(-1, 1)).flatten()\n",
        "            y_pred_best_rescaled = scaler_y.inverse_transform(y_pred_best_scaled.reshape(-1, 1)).flatten()\n",
        "            print(\"  Predictions and actual prices inverse-transformed to original scale.\")\n",
        "\n",
        "\n",
        "            plot_index = pd.to_datetime(final_data.index[-len(y_test_rescaled):])\n",
        "\n",
        "\n",
        "            plt.figure(figsize=(18, 9))\n",
        "            plt.plot(plot_index, y_test_rescaled, label='Actual Prices', color='blue', linewidth=2)\n",
        "            plt.plot(plot_index, y_pred_best_rescaled, label='Predicted Prices', color='red', linestyle='--', linewidth=2)\n",
        "            plt.title(f'{PRIMARY_STOCK} - Actual vs. Predicted Prices ({best_model_name} Optimized)', fontsize=18, fontweight='bold')\n",
        "            plt.xlabel('Date', fontsize=14)\n",
        "            plt.ylabel('Price ($)', fontsize=14)\n",
        "            plt.legend(fontsize=12)\n",
        "            plt.grid(True, linestyle=':', alpha=0.7)\n",
        "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "            plt.yticks(fontsize=10)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            print(f\"\\nFinal Performance Metrics for {best_model_name} (on Test Set):\")\n",
        "            print(f\"  Mean Squared Error (MSE): {model_results[best_model_name]['MSE']:.4f}\")\n",
        "            print(f\"  Mean Absolute Error (MAE): {model_results[best_model_name]['MAE']:.4f}\")\n",
        "            print(f\"  R-squared (R2): {model_results[best_model_name]['R2']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error during best model visualization: {e}\")\n",
        "            print(\"Please ensure your test data (X_test, y_test, X_test_series, y_test_series) and scaler_y are correctly defined and have consistent shapes.\")\n",
        "            print(\"Troubleshooting Tip: Check the dimensions of 'test_features' and how the model expects input.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No best model was identified or trained successfully in previous steps.\")\n",
        "\n",
        "else:\n",
        "    print(\" Required variables (model_results, trained_models, test data, scaler_y, etc.) not found. Please ensure Steps 1-5 are run correctly.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" Model Evaluation Performed!\")\n"
      ],
      "metadata": {
        "id": "tViloz3Uf2ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Iterative Training and Evaluation\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"This step is conceptual and depends on the performance observed in Step 6.\")\n",
        "print(\"It represents a feedback loop in the machine learning project lifecycle, allowing for refinements.\")\n",
        "\n",
        "MIN_ACCEPTABLE_R2 = 0.80\n",
        "\n",
        "# Ensure best_r2_score is available from Step 6\n",
        "if 'best_r2_score' in locals():\n",
        "    print(f\"Current best model's R2 Score: {best_r2_score:.4f}.\")\n",
        "\n",
        "    if best_r2_score < MIN_ACCEPTABLE_R2:\n",
        "        print(f\" Current best R2 ({best_r2_score:.4f}) is below the acceptable threshold ({MIN_ACCEPTABLE_R2}).\")\n",
        "    else:\n",
        "        print(f\" Model performance (R2: {best_r2_score:.4f}) is satisfactory (above {MIN_ACCEPTABLE_R2}).\")\n",
        "        print(\"The model is now ready for deployment and explainability analysis.\")\n",
        "else:\n",
        "    print(\" Best R2 score not available. Please ensure Step 6 was completed successfully.\")\n",
        "\n",
        "print(\"\\n Step 7 Complete: Iteration Decision Made!\")\n"
      ],
      "metadata": {
        "id": "ke2WMx_rf6fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Model Deployment\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ensure BEST_MODEL, feature_scaler, and scaler_y are available from previous steps\n",
        "if 'BEST_MODEL' in locals() and BEST_MODEL is not None and \\\n",
        "   'feature_scaler' in locals() and feature_scaler is not None and \\\n",
        "   'scaler_y' in locals() and scaler_y is not None:\n",
        "    print(\" Best model and scalers identified. Preparing for deployment.\")\n",
        "\n",
        "    model_type = \"unknown\"\n",
        "    model_save_path = \"\"\n",
        "\n",
        "    try:\n",
        "        # Save feature scaler\n",
        "        scaler_save_path = \"feature_scaler.joblib\"\n",
        "        joblib.dump(feature_scaler, scaler_save_path)\n",
        "        print(f\"  Feature scaler saved to: {scaler_save_path}.\")\n",
        "\n",
        "        # Save target scaler\n",
        "        scaler_y_save_path = \"target_scaler.joblib\"\n",
        "        joblib.dump(scaler_y, scaler_y_save_path)\n",
        "        print(f\"  Target scaler (for 'y') saved to: {scaler_y_save_path}.\")\n",
        "\n",
        "        # Save the best model based on its type\n",
        "        if isinstance(BEST_MODEL, Sequential) or \\\n",
        "           (hasattr(BEST_MODEL, 'model') and isinstance(BEST_MODEL.model, Sequential)):\n",
        "            model_save_path = \"best_deep_learning_model.h5\"\n",
        "            keras_model_to_save = BEST_MODEL.model if hasattr(BEST_MODEL, 'model') else BEST_MODEL\n",
        "            keras_model_to_save.save(model_save_path)\n",
        "            model_type = \"Deep Learning (Keras)\"\n",
        "        elif hasattr(BEST_MODEL, 'predict'):\n",
        "            model_save_path = \"best_traditional_ml_model.joblib\"\n",
        "            joblib.dump(BEST_MODEL, model_save_path)\n",
        "            model_type = \"Traditional ML (Scikit-learn/Joblib)\"\n",
        "        else:\n",
        "            print(\"   Could not determine model type for saving.\")\n",
        "            model_save_path = None\n",
        "\n",
        "        if model_save_path and os.path.exists(model_save_path):\n",
        "            print(f\"   Best model ({model_type}) saved successfully to: {model_save_path}.\")\n",
        "        elif model_save_path:\n",
        "            print(f\"   Model saving failed for {model_type}: File not found after saving attempt.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error during model saving or deployment considerations: {e}\")\n",
        "        print(\"Please ensure the BEST_MODEL object is a valid trained model and appropriate libraries (joblib, tensorflow) are installed.\")\n",
        "\n",
        "else:\n",
        "    print(\" No best model or scalers found to deploy. Please ensure Step 6 was completed successfully.\")\n",
        "\n",
        "print(\"\\n  Model Deployment Considerations Outlined!\")"
      ],
      "metadata": {
        "id": "mvO_DGnQgN2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n  Explainability\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"Making model predictions interpretable is crucial for building trust and providing insights, especially in financial domains.\")\n",
        "print(\"This step involves using explainability tools like SHAP (SHapley Additive exPlanations).\")\n",
        "\n",
        "\n",
        "\n",
        "if 'BEST_MODEL' in locals() and BEST_MODEL is not None and \\\n",
        "   'X_train' in locals() and 'X_test' in locals() and \\\n",
        "   'feature_names' in locals() and 'X_train_series' in locals() and 'X_test_series' in locals():\n",
        "    print(\" Best model, training data, test data, and feature names available for explainability analysis.\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        if X_train.shape[0] > 1000:\n",
        "            background_data_for_shap_traditional = X_train.sample(n=500, random_state=42)\n",
        "        else:\n",
        "            background_data_for_shap_traditional = X_train\n",
        "\n",
        "\n",
        "        if X_train_series.shape[0] > 1000:\n",
        "\n",
        "            random_indices = np.random.choice(X_train_series.shape[0], 500, replace=False)\n",
        "            background_data_for_shap_dl = X_train_series[random_indices]\n",
        "        else:\n",
        "            background_data_for_shap_dl = X_train_series\n",
        "\n",
        "        print(\"\\n Applying SHAP for Model Explainability:\")\n",
        "\n",
        "\n",
        "        if isinstance(BEST_MODEL, Sequential) or \\\n",
        "           (hasattr(BEST_MODEL, 'model') and isinstance(BEST_MODEL.model, Sequential)):\n",
        "\n",
        "            print(\"  - Using DeepExplainer for the deep learning model.\")\n",
        "\n",
        "\n",
        "            keras_model_for_shap = BEST_MODEL.model if hasattr(BEST_MODEL, 'model') else BEST_MODEL\n",
        "\n",
        "\n",
        "            shap_values_dl = []\n",
        "\n",
        "            batch_size = 500\n",
        "            for i in range(0, X_test_series.shape[0], batch_size):\n",
        "                shap_values_dl.append(explainer.shap_values(X_test_series[i:i + batch_size]))\n",
        "\n",
        "\n",
        "            if isinstance(shap_values_dl[0], list):\n",
        "                shap_values_dl = np.concatenate([s[0] for s in shap_values_dl], axis=0)\n",
        "            else:\n",
        "                shap_values_dl = np.concatenate(shap_values_dl, axis=0)\n",
        "\n",
        "\n",
        "            print(\"  SHAP values calculated for the Deep Learning model.\")\n",
        "\n",
        "            avg_shap_values = np.mean(shap_values_dl, axis=1)\n",
        "\n",
        "            dummy_X_test_df = pd.DataFrame(X_test_series[:, -1, :], columns=feature_names)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(avg_shap_values, dummy_X_test_df, plot_type=\"bar\", show=False, color='#2c7bb6')\n",
        "            plt.title(\"Global Feature Importance (SHAP Values) - Deep Learning (Averaged)\", fontsize=16)\n",
        "            plt.xticks(fontsize=10)\n",
        "            plt.yticks(fontsize=10)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure(figsize=(14, 9))\n",
        "            shap.summary_plot(avg_shap_values, dummy_X_test_df, show=False, plot_type=\"dot\", color_bar=True, color_bar_label=\"Feature Value\")\n",
        "            plt.title(\"SHAP Summary Plot - Deep Learning (Averaged)\", fontsize=16)\n",
        "            plt.xticks(fontsize=10)\n",
        "            plt.yticks(fontsize=10)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(\"\\n  Note: Individual prediction explanations (force plots) for deep learning models with sequential data require more specific handling of time series SHAP values.\")\n",
        "\n",
        "\n",
        "        elif hasattr(BEST_MODEL, 'feature_importances_') or isinstance(BEST_MODEL, (XGBRegressor, SVR)):\n",
        "            if isinstance(BEST_MODEL, (XGBRegressor)): # Tree-based models\n",
        "                print(\"  - Using TreeExplainer for the tree-based model.\")\n",
        "                explainer = shap.TreeExplainer(BEST_MODEL)\n",
        "            else: # For SVM, use KernelExplainer\n",
        "                print(\"  - Using KernelExplainer for the SVM model (can be slow for large datasets).\")\n",
        "                explainer = shap.KernelExplainer(BEST_MODEL.predict, background_data_for_shap_traditional)\n",
        "\n",
        "\n",
        "            X_test_df = X_test.copy() if isinstance(X_test, pd.DataFrame) else pd.DataFrame(X_test, columns=feature_names)\n",
        "\n",
        "\n",
        "            # Calculate SHAP values\n",
        "            shap_values = explainer.shap_values(X_test_df)\n",
        "            print(\"  SHAP values calculated.\")\n",
        "\n",
        "            # Visualize global feature importance (bar plot)\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, X_test_df, plot_type=\"bar\", show=False, color='#1b9e77')\n",
        "            plt.title(\"Global Feature Importance (SHAP Values)\", fontsize=16)\n",
        "            plt.xticks(fontsize=10)\n",
        "            plt.yticks(fontsize=10)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure(figsize=(14, 9))\n",
        "            shap.summary_plot(shap_values, X_test_df, show=False, plot_type=\"dot\", color_bar=True, color_bar_label=\"Feature Value\")\n",
        "            plt.title(\"SHAP Summary Plot\", fontsize=16)\n",
        "            plt.xticks(fontsize=10)\n",
        "            plt.yticks(fontsize=10)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(\"\\n  Individual Prediction Explanation (First Test Sample - Force Plot):\")\n",
        "            shap.initjs()\n",
        "\n",
        "            if isinstance(explainer.expected_value, np.ndarray) and explainer.expected_value.ndim > 0:\n",
        "                 expected_value_single = explainer.expected_value[0] if explainer.expected_value.ndim > 0 else explainer.expected_value\n",
        "            else:\n",
        "                 expected_value_single = explainer.expected_value\n",
        "            display(shap.force_plot(expected_value_single, shap_values[0,:], X_test_df.iloc[0,:]))\n",
        "\n",
        "        else:\n",
        "            print(\"  Explainability for this model type is not explicitly covered here with SHAP. Consider using KernelExplainer directly if applicable.\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\" SHAP library not installed. Please ensure you ran `!pip install shap` in the Project Setup block.\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error during SHAP explainability: {e}\")\n",
        "        print(\"Please ensure your data (X_test, X_train, feature_names, X_test_series, X_train_series) is correctly formatted and matches the model's input expectations.\")\n",
        "        print(\"For DeepExplainer, ensure X_train_series/X_test_series are 3D (samples, timesteps, features).\")\n",
        "\n",
        "else:\n",
        "    print(\" Required variables (BEST_MODEL, X_train, X_test, feature_names, X_train_series, X_test_series) not found. Please ensure Step 6 and relevant preceding steps are completed.\")\n",
        "\n",
        "print(\"\\n Step 9 Complete: Explainability Implemented (SHAP)!\")\n"
      ],
      "metadata": {
        "id": "1s9ZKvtQgZv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Step 10: Investor Dashboards\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if 'y_test_rescaled' in locals() and 'y_pred_best_rescaled' in locals() and \\\n",
        "   'PRIMARY_STOCK' in locals() and 'final_data' in locals():\n",
        "    print(\" Actual and predicted (rescaled) prices available for dashboard visualization.\")\n",
        "\n",
        "    plot_index_test = pd.to_datetime(final_data.index[-len(y_test_rescaled):])\n",
        "\n",
        "    # Dashboard Component 1: Actual vs. Predicted Prices Over Time\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.plot(plot_index_test, y_test_rescaled, label='Actual Stock Price', color='blue', linewidth=2, alpha=0.8)\n",
        "    plt.plot(plot_index_test, y_pred_best_rescaled, label='Predicted Stock Price', color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
        "    plt.title(f'{PRIMARY_STOCK} - Actual vs. Predicted Stock Prices', fontsize=20, fontweight='bold')\n",
        "    plt.xlabel('Date', fontsize=15)\n",
        "    plt.ylabel('Price ($)', fontsize=15)\n",
        "    plt.legend(fontsize=13)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Dashboard Component 2: Predicted vs. Actual Scatter Plot (for visualizing correlation)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(y_test_rescaled, y_pred_best_rescaled, alpha=0.6, color='purple', s=50, edgecolors='none') # Added size and edgecolors\n",
        "    # Plot a perfect prediction line (y=x)\n",
        "    min_val = min(y_test_rescaled.min(), y_pred_best_rescaled.min())\n",
        "    max_val = max(y_test_rescaled.max(), y_pred_best_rescaled.max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val],\n",
        "             linestyle='--', color='gray', linewidth=2, label='Perfect Prediction Line')\n",
        "    plt.title(f'{PRIMARY_STOCK} - Actual vs. Predicted Prices Scatter Plot', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Actual Price ($)', fontsize=14)\n",
        "    plt.ylabel('Predicted Price ($)', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Dashboard Component 3: Prediction Error Distribution\n",
        "    prediction_errors = y_test_rescaled - y_pred_best_rescaled\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(prediction_errors, bins=50, kde=True, color='teal', edgecolor='black')\n",
        "    plt.axvline(x=0, color='red', linestyle='--', label='Zero Error')\n",
        "    plt.title(f'{PRIMARY_STOCK} - Prediction Error Distribution', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Prediction Error ($)', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\" Not enough data (y_test_rescaled, y_pred_best_rescaled) to generate dashboard visualizations. Please ensure Step 6 was completed successfully.\")"
      ],
      "metadata": {
        "id": "Zoly_hORgq21"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3bXXxKjYFFrL1U6N0Tf9n"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}